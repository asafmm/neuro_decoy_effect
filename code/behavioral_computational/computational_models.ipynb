{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import scipy\n",
    "from scipy import optimize\n",
    "import read_files\n",
    "from copy import deepcopy\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=RuntimeWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "choice_stimuli = pd.read_csv('../../stimuli/stimuli_trinary.csv')\n",
    "trinary_stimuli = choice_stimuli\n",
    "binary_stimuli = pd.read_csv('../../stimuli/stimuli_binary.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit computational models: Adaptive Gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### min-max scaling of amount and probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_stimuli = trinary_stimuli.drop(['catch', 'decoyDirection', 'amountC', 'probC', 'trinary_id'], axis=1)\n",
    "binary_stimuli.loc[:, 'avg_amounts'] = np.mean(binary_stimuli.loc[:, ['amountA', 'amountB']], 1)\n",
    "binary_stimuli.loc[:, 'avg_probs'] = np.mean(binary_stimuli.loc[:, ['probA', 'probB']], 1)\n",
    "binary_stimuli = binary_stimuli.loc[binary_stimuli.binary_id < 28, :] # remove catch trials\n",
    "trinary_stimuli = trinary_stimuli.drop(['catch', 'decoyDirection', 'binary_id'], axis=1)\n",
    "trinary_stimuli.loc[:, 'avg_amounts'] = np.mean(trinary_stimuli.loc[:, ['amountA', 'amountB', 'amountC']], 1)\n",
    "trinary_stimuli.loc[:, 'avg_probs'] = np.mean(trinary_stimuli.loc[:, ['probA', 'probB', 'probC']], 1)\n",
    "trinary_stimuli = trinary_stimuli.loc[trinary_stimuli.trinary_id < 28] # remove catch trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_amount = 4\n",
    "max_amount = 79\n",
    "min_prob = 0\n",
    "max_prob = 100\n",
    "all_EVs = np.concatenate([trinary_stimuli.EVA.unique(), trinary_stimuli.EVB.unique(), trinary_stimuli.EVC.unique()])\n",
    "min_EV = np.min(all_EVs)\n",
    "max_EV = np.max(all_EVs)\n",
    "to_scale_cols = ['amountA', 'probA', 'amountB', 'probB', 'amountC', 'probC', 'EVA', 'EVB', 'EVC', 'avg_amounts', 'avg_probs']\n",
    "trinary_stimuli_norm = trinary_stimuli.copy()\n",
    "trinary_stimuli_norm.loc[:, 'amountA'] = (trinary_stimuli_norm.amountA - min_amount) / (max_amount - min_amount)\n",
    "trinary_stimuli_norm.loc[:, 'amountB'] = (trinary_stimuli_norm.amountB - min_amount) / (max_amount - min_amount)\n",
    "trinary_stimuli_norm.loc[:, 'amountC'] = (trinary_stimuli_norm.amountC - min_amount) / (max_amount - min_amount)\n",
    "trinary_stimuli_norm.loc[:, 'probA'] = (trinary_stimuli_norm.probA - min_prob) / (max_prob - min_prob)\n",
    "trinary_stimuli_norm.loc[:, 'probB'] = (trinary_stimuli_norm.probB - min_prob) / (max_prob - min_prob)\n",
    "trinary_stimuli_norm.loc[:, 'probC'] = (trinary_stimuli_norm.probC - min_prob) / (max_prob - min_prob)\n",
    "trinary_stimuli_norm.loc[:, 'EVA'] = (trinary_stimuli_norm.EVA - min_EV) / (max_EV - min_EV)\n",
    "trinary_stimuli_norm.loc[:, 'EVB'] = (trinary_stimuli_norm.EVB - min_EV) / (max_EV - min_EV)\n",
    "trinary_stimuli_norm.loc[:, 'EVC'] = (trinary_stimuli_norm.EVC - min_EV) / (max_EV - min_EV)\n",
    "trinary_stimuli_norm.loc[:, 'avg_amounts'] = trinary_stimuli_norm.loc[:, ['amountA', 'amountB', 'amountC']].mean(axis=1)\n",
    "trinary_stimuli_norm.loc[:, 'avg_probs'] = trinary_stimuli_norm.loc[:, ['probA', 'probB', 'probC']].mean(axis=1)\n",
    "\n",
    "binary_stimuli_norm = binary_stimuli.copy()\n",
    "binary_stimuli_norm.loc[:, 'amountA'] = (binary_stimuli_norm.amountA - min_amount) / (max_amount - min_amount)\n",
    "binary_stimuli_norm.loc[:, 'amountB'] = (binary_stimuli_norm.amountB - min_amount) / (max_amount - min_amount)\n",
    "binary_stimuli_norm.loc[:, 'probA'] = (binary_stimuli_norm.probA - min_prob) / (max_prob - min_prob)\n",
    "binary_stimuli_norm.loc[:, 'probB'] = (binary_stimuli_norm.probB - min_prob) / (max_prob - min_prob)\n",
    "binary_stimuli_norm.loc[:, 'EVA'] = (binary_stimuli_norm.EVA - min_EV) / (max_EV - min_EV)\n",
    "binary_stimuli_norm.loc[:, 'EVB'] = (binary_stimuli_norm.EVB - min_EV) / (max_EV - min_EV)\n",
    "binary_stimuli_norm.loc[:, 'avg_amounts'] = binary_stimuli_norm.loc[:, ['amountA', 'amountB']].mean(axis=1)\n",
    "binary_stimuli_norm.loc[:, 'avg_probs'] = binary_stimuli_norm.loc[:, ['probA', 'probB']].mean(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### read choice file per set, instead of per subject\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_choices = pd.DataFrame()\n",
    "raw_files = glob.glob('../../data/behavioral_experiment/raw_choices_per_set/*.csv')\n",
    "for i in range(len(raw_files)):\n",
    "    set_choices = pd.read_csv(raw_files[i])\n",
    "    raw_choices = pd.concat([raw_choices, set_choices], axis=0)\n",
    "# remove catch trials\n",
    "raw_choices = raw_choices[raw_choices.binary_id<28]\n",
    "raw_choices = raw_choices[raw_choices.subject_id.str.contains('-')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the lotteries' attributes to the choices\n",
    "binary_raw_choices = raw_choices[raw_choices.trinary_group==0]\n",
    "trinary_raw_choices = raw_choices[raw_choices.trinary_group==1]\n",
    "trinary_raw_choices_stimuli = trinary_raw_choices.merge(trinary_stimuli_norm, left_on='trinary_id', right_on='trinary_id', how='outer')\n",
    "binary_raw_choices_stimuli = binary_raw_choices.merge(binary_stimuli_norm, left_on='binary_id', right_on='binary_id', how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_choices_stimuli = pd.concat([binary_raw_choices_stimuli, trinary_raw_choices_stimuli], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(params, df, return_p=0):\n",
    "    # df could be subjects' raw choices, or dataframe of stimuli (amounts, probs) without choices\n",
    "    if 'amountC' in df.columns:\n",
    "        # if trinary group, include option C\n",
    "        amounts = df[['amountA', 'amountB', 'amountC']]\n",
    "        probs = df[['probA', 'probB', 'probC']]\n",
    "    else:\n",
    "        # if binary group, include only A and B\n",
    "        amounts = df[['amountA', 'amountB']]\n",
    "        probs = df[['probA', 'probB']]\n",
    "    n_amounts = amounts.shape[1]\n",
    "    avg_amounts_mat = np.vstack([df.avg_amounts.values]*n_amounts).transpose()\n",
    "    avg_probs_mat = np.vstack([df.avg_probs.values]*n_amounts).transpose()\n",
    "\n",
    "    # adaptive gain\n",
    "    c_amount, c_prob, tau, slope, w = params\n",
    "    u_amounts = 1 / (1 + np.exp(-1 * (amounts - avg_amounts_mat - c_amount) / slope))\n",
    "    u_probs =  1 / (1 + np.exp(-1 * (probs - avg_probs_mat - c_prob) / slope))\n",
    "\n",
    "    u_all = w*u_amounts.values + (1-w)*u_probs.values\n",
    "    exp_u = np.exp(tau*u_all)\n",
    "    softmax_denominator = np.vstack([np.nansum(exp_u, axis=1)]*n_amounts).transpose()\n",
    "    \n",
    "    p_all = exp_u / softmax_denominator\n",
    "    p_A = p_all[:, 0] / (p_all[:, 0] + p_all[:, 1])\n",
    "    p_B = p_all[:, 1] / (p_all[:, 0] + p_all[:, 1])\n",
    "    p_A[p_A==0] = 1e-5\n",
    "    p_B[p_B==0] = 1e-5\n",
    "    if return_p:\n",
    "        # return probaility to choose A, for inference\n",
    "        return p_A\n",
    "    if 'target_choice' in df.columns:\n",
    "        # return negative log likelihood, for optimization\n",
    "        choose_A = df.target_choice.values\n",
    "        neg_log_like = -1 * np.sum(choose_A * np.log(p_A) + (1-choose_A)*np.log(p_B))\n",
    "        return neg_log_like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### out of sample predictions with AG (leave-one-set-out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../mri')\n",
    "from utils import load_params\n",
    "behavior_results = load_params.load_behavior_results()\n",
    "set_objs = load_params.load_sets(behavior_results)\n",
    "folds = []\n",
    "for i, set_out in enumerate(set_objs):\n",
    "    test_sets = []\n",
    "    test_ind = []\n",
    "    train_sets = []\n",
    "    train_ind = []\n",
    "    for j, set_obj in enumerate(set_objs):\n",
    "        if set_obj.overlapping_with(set_out):\n",
    "            test_sets.append(set_obj)\n",
    "            test_ind.append(j)\n",
    "        else:\n",
    "            train_sets.append(set_obj)\n",
    "            train_ind.append(j)\n",
    "    if (train_ind, test_ind) not in folds:\n",
    "        folds.append((train_ind, test_ind))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 1/25 [00:01<00:42,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 4994.638847\n",
      "         Iterations: 494\n",
      "         Function evaluations: 788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 2/25 [00:04<00:51,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 4932.161411\n",
      "         Iterations: 676\n",
      "         Function evaluations: 1103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 3/25 [00:07<00:57,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 4699.147492\n",
      "         Iterations: 863\n",
      "         Function evaluations: 1382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 4/25 [00:09<00:49,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 4693.421686\n",
      "         Iterations: 537\n",
      "         Function evaluations: 855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 5/25 [00:11<00:42,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 4809.875084\n",
      "         Iterations: 475\n",
      "         Function evaluations: 773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 6/25 [00:13<00:41,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 5411.537233\n",
      "         Iterations: 614\n",
      "         Function evaluations: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 7/25 [00:14<00:35,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 5012.209351\n",
      "         Iterations: 444\n",
      "         Function evaluations: 709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 8/25 [00:17<00:34,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 5834.963057\n",
      "         Iterations: 568\n",
      "         Function evaluations: 917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 9/25 [00:18<00:31,  1.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 5262.705480\n",
      "         Iterations: 480\n",
      "         Function evaluations: 761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 10/25 [00:21<00:31,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 4941.596022\n",
      "         Iterations: 659\n",
      "         Function evaluations: 1074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 11/25 [00:22<00:26,  1.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 4927.100801\n",
      "         Iterations: 368\n",
      "         Function evaluations: 622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 12/25 [00:26<00:32,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 5090.076712\n",
      "         Iterations: 994\n",
      "         Function evaluations: 1643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 13/25 [00:28<00:28,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 4812.363828\n",
      "         Iterations: 637\n",
      "         Function evaluations: 1003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 14/25 [00:30<00:24,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 4769.018360\n",
      "         Iterations: 485\n",
      "         Function evaluations: 775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 15/25 [00:31<00:20,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 5054.119459\n",
      "         Iterations: 401\n",
      "         Function evaluations: 636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 16/25 [00:34<00:18,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 4572.141469\n",
      "         Iterations: 582\n",
      "         Function evaluations: 943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 17/25 [00:35<00:13,  1.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 4947.659993\n",
      "         Iterations: 264\n",
      "         Function evaluations: 450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 18/25 [00:37<00:12,  1.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 4661.264897\n",
      "         Iterations: 625\n",
      "         Function evaluations: 981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 19/25 [00:40<00:14,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 4289.047116\n",
      "         Iterations: 968\n",
      "         Function evaluations: 1574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 20/25 [00:43<00:11,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 4925.119664\n",
      "         Iterations: 617\n",
      "         Function evaluations: 1001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 21/25 [00:44<00:08,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 4510.816397\n",
      "         Iterations: 509\n",
      "         Function evaluations: 829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 22/25 [00:47<00:06,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 4844.304055\n",
      "         Iterations: 668\n",
      "         Function evaluations: 1083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 23/25 [00:50<00:04,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 4663.337854\n",
      "         Iterations: 810\n",
      "         Function evaluations: 1273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 24/25 [00:51<00:02,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 5212.572638\n",
      "         Iterations: 386\n",
      "         Function evaluations: 638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:53<00:00,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 4894.390753\n",
      "         Iterations: 531\n",
      "         Function evaluations: 849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cv_rmse = np.zeros(len(folds))\n",
    "cv_corr = np.zeros(len(folds))\n",
    "train_rmse = np.zeros(len(folds))\n",
    "c_amount_init = 0\n",
    "c_prob_init = 0\n",
    "tau_init = 0.15\n",
    "slope_init = 0.1\n",
    "w_init = 0.3\n",
    "\n",
    "fold_i = 0\n",
    "for train_ind, test_ind in tqdm.tqdm(folds):\n",
    "    # fit model to training sets\n",
    "    y_train, y_test = y[train_ind], y[test_ind]\n",
    "    train_set_inds = np.array(train_ind) + 1\n",
    "    test_set_inds = np.array(test_ind) + 1\n",
    "    train_raw_choices = raw_choices_stimuli[raw_choices_stimuli.trinary_id.isin(train_set_inds)]\n",
    "    test_raw_choices = raw_choices_stimuli[raw_choices_stimuli.trinary_id.isin(test_set_inds)]\n",
    "    AG_group_params = optimize.fmin(run_model, AG_init_params, args=(train_raw_choices, 0), maxiter=1e4, disp=True)\n",
    "    # calculate prediction for choice ratios for trinary and binary test sets\n",
    "    test_trinary_stimuli = trinary_stimuli_norm.loc[trinary_stimuli_norm.trinary_id.isin(test_set_inds)]\n",
    "    test_binary_stimuli = binary_stimuli_norm.loc[binary_stimuli_norm.binary_id.isin(test_set_inds)]\n",
    "    trinary_ratio_pred = run_model(AG_group_params, test_trinary_stimuli, return_p=1)\n",
    "    binary_ratio_pred = run_model(AG_group_params, test_binary_stimuli, return_p=1)\n",
    "    # predicted decoy effect is predicted trinary ratio minus predicted binary ratio\n",
    "    decoy_pred = trinary_ratio_pred - binary_ratio_pred\n",
    "    decoy_pred = np.nan_to_num(decoy_pred)\n",
    "    cv_rmse[fold_i] = np.sqrt(mean_squared_error(y_test, decoy_pred))\n",
    "    corr = scipy.stats.spearmanr(decoy_pred, y_test)[0]\n",
    "    cv_corr[fold_i] = corr if ~np.isnan(corr) else 0\n",
    "    # calculate prediction for choice ratios for trinary and binary training sets\n",
    "    train_trinary_stimuli = trinary_stimuli_norm.loc[trinary_stimuli_norm.trinary_id.isin(train_set_inds)]\n",
    "    train_binary_stimuli = binary_stimuli_norm.loc[binary_stimuli_norm.binary_id.isin(train_set_inds)]\n",
    "    train_trinary_ratio_pred = run_model(AG_group_params, train_trinary_stimuli, return_p=1)\n",
    "    train_binary_ratio_pred = run_model(AG_group_params, train_binary_stimuli, return_p=1)\n",
    "    train_decoy_pred = train_trinary_ratio_pred - train_binary_ratio_pred\n",
    "    train_rmse[fold_i] = np.sqrt(mean_squared_error(y_train, train_decoy_pred))\n",
    "    fold_i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11180057258869"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.mean(cv_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2751301537428308"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.mean(cv_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fitting with AG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 6512.408070\n",
      "         Iterations: 482\n",
      "         Function evaluations: 771\n"
     ]
    }
   ],
   "source": [
    "c_amount_init = 0\n",
    "c_prob_init = 0\n",
    "tau_init = 0.15\n",
    "slope_init = 0.1\n",
    "w_init = 0.3\n",
    "AG_init_params = np.array([c_amount_init, c_prob_init, tau_init, slope_init, w_init])\n",
    "AG_group_params = optimize.fmin(run_model, AG_init_params, args=(raw_choices_stimuli, 0), maxiter=1e4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.21432596,  0.03485916,  2.60392595,  0.03045105,  0.73974457])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AG_group_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_decoy_effects = pd.read_csv('../../results/decoy_table.csv')\n",
    "y = real_decoy_effects.decoy_effect_A.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "trinary_P_a = run_model(AG_group_params, trinary_stimuli_norm, return_p=1)\n",
    "binary_P_a = run_model(AG_group_params, binary_stimuli_norm, return_p=1)\n",
    "decoys = trinary_P_a - binary_P_a\n",
    "AG_decoys = pd.DataFrame({'trinary_id': trinary_stimuli.trinary_id, 'P_a_trinary': trinary_P_a, 'P_a_binary': binary_P_a, 'decoy_AG':decoys})\n",
    "AG_decoys = AG_decoys.merge(real_decoy_effects[['trinary_id', 'decoy_effect_A']], on='trinary_id').sort_values('decoy_effect_A')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7384623403900014"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AG_decoys[['decoy_AG', 'decoy_effect_A']].corr().values[0, 1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "decoy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
